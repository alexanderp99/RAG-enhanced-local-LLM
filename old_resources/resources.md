// This file shows useful resources

[LLM prompt format make or break your LLM and RAG](https://www.youtube.com/watch?v=M5i3rQfEw_A)

[Why your RAG is not working](https://medium.com/@saurabhgssingh/why-your-rag-is-not-working-96053b4d5305)

[Hybrid Search!](https://medium.com/@arunhara/supercharge-your-search-with-chromadb-94455dcbba2f)

[Safety prompts](https://safetyprompts.com/)

[LLM derived knowledge Graphs for RAG](https://www.youtube.com/watch?v=r09tJfON6kE)

[Using LLM Chain](https://github.com/langchain-ai/langchain/discussions/15680)

[LLama3 instruction fine tuned(trellis)](https://huggingface.co/Trelis/Meta-Llama-3-8B-Instruct-function-calling)

[Tiny llama finetuning guide](https://app.gumroad.com/d/0d062e4af9421fa9b54698aaf6aa065c)

[Mistral 7B Fine-Tuning Guide ](https://app.gumroad.com/d/9456144f169533699fa57c85fea3d182)

[Video,Function Calling Datasets, Training and Inference
](https://www.youtube.com/watch?v=hHn_cV5WUDI)

[AYA- llm 33 languages](https://ollama.com/library/aya)

[Prompting Techniques](https://www.promptingguide.ai/techniques)

[LLM agent architectures](https://www.youtube.com/watch?v=ZJlfF1ESXVw&t=160s)

[Huggingface LangChainHub Prompts](https://huggingface.co/datasets/LangChainHub-Prompts/LLM_Math?row=0)

[Reliable, fully local RAG agents with LLaMA3](https://www.youtube.com/watch?app=desktop&si=zMrBmAF0NSmMfbvS&v=-ROS6gfYIts&feature=youtu.be)

[Instruction Principles for LLMs](https://arxiv.org/pdf/2312.16171v2)

[LLM AUGMENTED LLMS: EXPANDING CAPABILITIES THROUGH COMPOSITION](https://arxiv.org/pdf/2401.02412)

[JSON-based Agents With Ollama & LangChain](https://medium.com/neo4j/json-based-agents-with-ollama-langchain-9cf9ab3c84ef)

[Graph of thoughts](https://arxiv.org/abs/2308.09687?source=post_page-----9ae7ef1af4a4--------------------------------)

[Ollama chatprompt, system api params](https://blog.xentoo.info/2024/03/18/ollama-system-prompt/)

[REACT Agent in langchain](https://api.python.langchain.com/en/latest/agents/langchain.agents.react.agent.create_react_agent.html)

[Very good/extensive langgraph tutorial](https://langchain-ai.github.io/langgraph/tutorials/usaco/usaco/)

[Generating UML python](https://stackoverflow.com/questions/77421030/how-to-generate-the-uml-diagram-from-the-python-code)

[ADVAnCED PROMPTING DSPY](https://github.com/stanfordnlp/dspy)

[Function Calling LLAma](https://www.youtube.com/watch?app=desktop&si=PA6LaFg-8txoKgPc&v=9K51Leyv3qI&feature=youtu.be)

[Getting the most out of your tokenizer](https://arxiv.org/pdf/2402.01035)

[Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models
](https://arxiv.org/abs/2403.00417)

[Special Tokens](https://www.reddit.com/r/LocalLLaMA/comments/17n22vk/troubleshooting_special_tokens_in_transformer/)

[Global attention token](https://arxiv.org/pdf/2209.08698)

[Better steering LLM Agents with LMQL](https://vivien000.github.io/blog/journal/better-steering-LLM-agents-with-LMQL.html)

[Prompting is programming](https://arxiv.org/abs/2212.06094)

[Furgal GPT](https://arxiv.org/abs/2305.05176)

[Matryoshka representation Learning](Matryoshka Representation Learning)

[Retrieval Head Mechanistically Explains Long-Context Factuality](https://arxiv.org/abs/2404.15574)

[Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets
](https://arxiv.org/abs/2201.02177)

[Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization](https://arxiv.org/abs/2405.15071)

[Grokfast: Accelerated Grokking by Amplifying Slow Gradients
](https://arxiv.org/abs/2405.20233)

[The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More
](https://arxiv.org/abs/2406.05183)



